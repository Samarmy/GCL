{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac48dbc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(1, os.path.join(sys.path[0], \"..\"))\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from argparse import ArgumentParser\n",
    "import torch\n",
    "\n",
    "import data_utils.utils as data_utils\n",
    "import inference.utils as inference_utils\n",
    "import BigGAN_PyTorch.utils as biggan_utils\n",
    "from data_utils.datasets_common import pil_loader\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "from glob import glob\n",
    "import h5py as h5\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93760952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(root_path, model, resolution, which_dataset, visualize_instance_images):\n",
    "    data_path = os.path.join(root_path, \"stored_instances\")\n",
    "    if model == \"cc_icgan\":\n",
    "        feature_extractor = \"classification\"\n",
    "    else:\n",
    "        feature_extractor = \"selfsupervised\"\n",
    "    filename = \"%s_res%i_rn50_%s_kmeans_k1000_instance_features.npy\" % (\n",
    "        which_dataset,\n",
    "        resolution,\n",
    "        feature_extractor,\n",
    "    )\n",
    "    # Load conditioning instances from files\n",
    "    data = np.load(os.path.join(data_path, filename), allow_pickle=True).item()\n",
    "\n",
    "    transform_list = None\n",
    "    if visualize_instance_images:\n",
    "        # Transformation used for ImageNet images.\n",
    "        transform_list = transforms.Compose(\n",
    "            [data_utils.CenterCropLongEdge(), transforms.Resize(resolution)]\n",
    "        )\n",
    "    return data, transform_list\n",
    "\n",
    "\n",
    "def get_model(exp_name, root_path, backbone, device=\"cuda\"):\n",
    "    parser = biggan_utils.prepare_parser()\n",
    "    parser = biggan_utils.add_sample_parser(parser)\n",
    "    parser = inference_utils.add_backbone_parser(parser)\n",
    "\n",
    "    args = [\"--experiment_name\", exp_name]\n",
    "    args += [\"--base_root\", root_path]\n",
    "    args += [\"--model_backbone\", backbone]\n",
    "\n",
    "    config = vars(parser.parse_args(args=args))\n",
    "\n",
    "    # Load model and overwrite configuration parameters if stored in the model\n",
    "    config = biggan_utils.update_config_roots(config, change_weight_folder=False)\n",
    "    generator, config = inference_utils.load_model_inference(config, device=device)\n",
    "    biggan_utils.count_parameters(generator)\n",
    "    generator.eval()\n",
    "\n",
    "    return generator\n",
    "\n",
    "\n",
    "def get_conditionings(test_config, generator, data):\n",
    "    # Obtain noise vectors\n",
    "    z = torch.empty(\n",
    "        5 * 5,\n",
    "        generator.z_dim if \"biggan\" == \"stylegan2\" else generator.dim_z,\n",
    "    ).normal_(mean=0, std=1.0)\n",
    "\n",
    "    # Subsampling some instances from the 1000 k-means centers file\n",
    "    if 5 > 1:\n",
    "        total_idxs = np.random.choice(\n",
    "            range(1000), 5, replace=False\n",
    "        )\n",
    "\n",
    "    # Obtain features, labels and ground truth image paths\n",
    "    all_feats, all_img_paths, all_labels = [], [], []\n",
    "    for counter in range(5):\n",
    "        # Index in 1000 k-means centers file\n",
    "        if None is not None:\n",
    "            idx = None\n",
    "        else:\n",
    "            idx = total_idxs[counter]\n",
    "        # Image paths to visualize ground-truth instance\n",
    "        if False:\n",
    "            all_img_paths.append(data[\"image_path\"][idx])\n",
    "        # Instance features\n",
    "        all_feats.append(\n",
    "            torch.FloatTensor(data[\"instance_features\"][idx : idx + 1]).repeat(\n",
    "                5, 1\n",
    "            )\n",
    "        )\n",
    "        # Obtain labels\n",
    "        if None is not None:\n",
    "            # Swap label for a manually specified one\n",
    "            label_int = None\n",
    "        else:\n",
    "            # Use the label associated to the instance feature\n",
    "            label_int = int(data[\"labels\"][idx])\n",
    "        # Format labels according to the backbone\n",
    "        labels = None\n",
    "        if \"biggan\" == \"stylegan2\":\n",
    "            dim_labels = 1000\n",
    "            labels = torch.eye(dim_labels)[torch.LongTensor([label_int])].repeat(\n",
    "                5, 1\n",
    "            )\n",
    "        else:\n",
    "            if \"icgan\" == \"cc_icgan\":\n",
    "                labels = torch.LongTensor([label_int]).repeat(\n",
    "                    5\n",
    "                )\n",
    "        all_labels.append(labels)\n",
    "    # Concatenate all conditionings\n",
    "    all_feats = torch.cat(all_feats)\n",
    "    if all_labels[0] is not None:\n",
    "        all_labels = torch.cat(all_labels)\n",
    "    else:\n",
    "        all_labels = None\n",
    "    return z, all_feats, all_labels, all_img_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e8149e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### -- Data -- ###\n",
    "data, transform_list = get_data(\n",
    "    \"/Work2/Watch_This/ICGAN/ic_gan/pretrained_models_path\",\n",
    "    \"icgan\",\n",
    "    256,\n",
    "    \"imagenet\",\n",
    "    False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84eb9ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "exp_name = \"%s_%s_%s_res%i%s\" % (\n",
    "        \"icgan\",\n",
    "        \"biggan\",\n",
    "        \"imagenet\",\n",
    "        256,\n",
    "        \"\",\n",
    "    )\n",
    "generator = get_model(\n",
    "        exp_name, \"/Work2/Watch_This/ICGAN/ic_gan/pretrained_models_path\", \"biggan\", device=device\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7323141",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29d3eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(torch.tensor(data['instance_features'][0]), \"/Work2/Watch_This/ICGAN/ic_gan/data_utils/test_images/0_feat_map.pth\")\n",
    "torch.save(torch.tensor(data['instance_features'][1]), \"/Work2/Watch_This/ICGAN/ic_gan/data_utils/test_images/1_feat_map.pth\")\n",
    "torch.save(torch.tensor(data['instance_features'][2]), \"/Work2/Watch_This/ICGAN/ic_gan/data_utils/test_images/2_feat_map.pth\")\n",
    "torch.save(torch.tensor(data['instance_features'][3]), \"/Work2/Watch_This/ICGAN/ic_gan/data_utils/test_images/3_feat_map.pth\")\n",
    "torch.save(torch.tensor(data['instance_features'][4]), \"/Work2/Watch_This/ICGAN/ic_gan/data_utils/test_images/4_feat_map.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab32f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open(\"/Work1/imagenet/\" + data['image_path'][0]).save(\"/Work2/Watch_This/ICGAN/ic_gan/data_utils/test_images/0_img.JPEG\") \n",
    "Image.open(\"/Work1/imagenet/\" + data['image_path'][1]).save(\"/Work2/Watch_This/ICGAN/ic_gan/data_utils/test_images/1_img.JPEG\") \n",
    "Image.open(\"/Work1/imagenet/\" + data['image_path'][2]).save(\"/Work2/Watch_This/ICGAN/ic_gan/data_utils/test_images/2_img.JPEG\") \n",
    "Image.open(\"/Work1/imagenet/\" + data['image_path'][3]).save(\"/Work2/Watch_This/ICGAN/ic_gan/data_utils/test_images/3_img.JPEG\") \n",
    "Image.open(\"/Work1/imagenet/\" + data['image_path'][4]).save(\"/Work2/Watch_This/ICGAN/ic_gan/data_utils/test_images/4_img.JPEG\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf941e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "z.shape, None, all_feats[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9482fc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "z, all_feats, all_labels, all_img_paths = get_conditionings(\n",
    "        None, generator, data\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79d6ecb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01786717 0.03970662 0.00147569 ... 0.00897138 0.00036234 0.07025905] b'/Work1/ima'\n"
     ]
    }
   ],
   "source": [
    "with h5.File(\"/Work2/Watch_This/ICGAN/ic_gan/data/ILSVRC256_feats_selfsupervised_resnet50.hdf5\", \"r\") as f:\n",
    "#     augment_data =(f[\"feats\"].clone(), f[\"paths\"].clone())\n",
    "    for i in range(f[\"feats\"].shape[0]):\n",
    "        print(f[\"feats\"][i], f[\"paths\"][i])\n",
    "        break\n",
    "#     feat_data = torch.tensor(f[\"feats\"][:10])\n",
    "#     print(feat_data.shape)\n",
    "    feat_input, feat_path = f[\"feats\"][0], f[\"paths\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c735403a",
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f2479a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feat_input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m input_feat_map \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43mfeat_input\u001b[49m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m      2\u001b[0m input_feat_map \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(input_feat_map, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'feat_input' is not defined"
     ]
    }
   ],
   "source": [
    "input_feat_map = torch.tensor(feat_input).unsqueeze(0).float()\n",
    "input_feat_map /= np.linalg.norm(input_feat_map, axis=1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419bc247",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_img1 = generator(\n",
    "            z[0:1].float(), None, input_feat_map\n",
    "        )\n",
    "out_img1 = torch.clamp((out_img1 * 0.5 + 0.5), 0, 1)\n",
    "torchvision.transforms.functional.to_pil_image(out_img1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb7bca7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7671759",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a83ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced289e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_generated_images = []\n",
    "with torch.no_grad():\n",
    "    num_batches = 1 + (z.shape[0]) // 16\n",
    "    for i in range(num_batches):\n",
    "        start = 16 * i\n",
    "        end = min(\n",
    "            16 * i + 16, z.shape[0]\n",
    "        )\n",
    "        if all_labels is not None:\n",
    "            labels_ = all_labels[start:end].to(device)\n",
    "        else:\n",
    "            labels_ = None\n",
    "        gen_img = generator(\n",
    "            z[start:end].to(device), labels_, all_feats[start:end].to(device)\n",
    "        )\n",
    "        if \"biggan\" == \"biggan\":\n",
    "            gen_img = ((gen_img * 0.5 + 0.5) * 255).int()\n",
    "        elif \"biggan\" == \"stylegan2\":\n",
    "            gen_img = torch.clamp((gen_img * 127.5 + 128), 0, 255).int()\n",
    "        all_generated_images.append(gen_img.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d480fca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_generated_images = torch.cat(all_generated_images)\n",
    "all_generated_images = all_generated_images.permute(0, 2, 3, 1).numpy()\n",
    "\n",
    "big_plot = []\n",
    "for i in range(0, 5):\n",
    "    row = []\n",
    "    for j in range(0, 5):\n",
    "        subplot_idx = (i * 5) + j\n",
    "        row.append(all_generated_images[subplot_idx])\n",
    "    row = np.concatenate(row, axis=1)\n",
    "    big_plot.append(row)\n",
    "big_plot = np.concatenate(big_plot, axis=0)\n",
    "\n",
    "# (Optional) Show ImageNet ground-truth conditioning instances\n",
    "# if False:\n",
    "#     all_gt_imgs = []\n",
    "#     for i in range(0, len(all_img_paths)):\n",
    "#         all_gt_imgs.append(\n",
    "#             np.array(\n",
    "#                 transform_list(\n",
    "#                     pil_loader(\n",
    "#                         os.path.join(test_config[\"dataset_path\"], all_img_paths[i])\n",
    "#                     )\n",
    "#                 )\n",
    "#             ).astype(np.uint8)\n",
    "#         )\n",
    "#     all_gt_imgs = np.concatenate(all_gt_imgs, axis=0)\n",
    "#     white_space = (\n",
    "#         np.ones((all_gt_imgs.shape[0], 20, all_gt_imgs.shape[2])) * 255\n",
    "#     ).astype(np.uint8)\n",
    "#     big_plot = np.concatenate([all_gt_imgs, white_space, big_plot], axis=1)\n",
    "\n",
    "plt.figure(\n",
    "    figsize=(\n",
    "        5 * 5,\n",
    "        5 * 5,\n",
    "    )\n",
    ")\n",
    "plt.imshow(big_plot)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "fig_path = \"%s_Generations_with_InstanceDataset_%s%s%s_zvar%0.2f.png\" % (\n",
    "    exp_name,\n",
    "    \"imagenet\",\n",
    "    \"_index\" + str(None)\n",
    "    if None is not None\n",
    "    else \"\",\n",
    "    \"_class_idx\" + str(None)\n",
    "    if None is not None\n",
    "    else \"\",\n",
    "    1.0,\n",
    ")\n",
    "plt.savefig(fig_path, dpi=600, bbox_inches=\"tight\", pad_inches=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47de3d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54540c3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
