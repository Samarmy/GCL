{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65ab5bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faiss library not found!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(1, os.path.join(sys.path[0], \"..\"))\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from argparse import ArgumentParser\n",
    "import torch\n",
    "\n",
    "import data_utils.utils as data_utils\n",
    "import inference.utils as inference_utils\n",
    "import BigGAN_PyTorch.utils as biggan_utils\n",
    "from data_utils.datasets_common import pil_loader\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from glob import glob\n",
    "import utils\n",
    "import os\n",
    "from PIL import Image\n",
    "from data_utils.resnet import resnet50\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "from metrics import metric_utils\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ebd865d-6042-4157-8513-8f5375f15188",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitty = 2\n",
    "torch.cuda.set_device(splitty)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(splitty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2da7a7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(exp_name, root_path, backbone, device=\"cuda\"):\n",
    "    parser = biggan_utils.prepare_parser()\n",
    "    parser = biggan_utils.add_sample_parser(parser)\n",
    "    parser = inference_utils.add_backbone_parser(parser)\n",
    "\n",
    "    args = [\"--experiment_name\", exp_name]\n",
    "    args += [\"--base_root\", root_path]\n",
    "    args += [\"--model_backbone\", backbone]\n",
    "\n",
    "    config = vars(parser.parse_args(args=args))\n",
    "\n",
    "    # Load model and overwrite configuration parameters if stored in the model\n",
    "    config = biggan_utils.update_config_roots(config, change_weight_folder=False)\n",
    "    generator, config = inference_utils.load_model_inference(config, device=device)\n",
    "    biggan_utils.count_parameters(generator)\n",
    "    generator.eval()\n",
    "\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae0c4b6a-de2b-4f17-8f70-a3af626d6ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = utils.load_pretrained_feature_extractor(\n",
    "#         '/Work2/Watch_This/ICGAN/ic_gan/pretrained_models_path/swav_800ep_pretrain.pth.tar',\n",
    "#         \"selfsupervised\",\n",
    "#         \"resnet50\",\n",
    "#     )\n",
    "# # net = torch.nn.DataParallel(net)\n",
    "# net.to(\"cuda\")\n",
    "# net.eval()\n",
    "# norm_mean = torch.Tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(\"cuda\")\n",
    "# norm_std = torch.Tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32bdd0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pegging all root folders to base root /Work2/Watch_This/ICGAN/ic_gan/pretrained_models_path\n",
      "For name best  best0  we have an FID:  22.453704833984375\n",
      "Checkpoint with name  best1  not in folder.\n",
      "Final name selected is  best0\n",
      "Loading best0 weights from /Work2/Watch_This/ICGAN/ic_gan/pretrained_models_path/icgan_biggan_imagenet_res256...\n",
      "Experiment name is icgan_biggan_imagenet_res256\n",
      "Adding attention layer in G at resolution 64\n",
      "Param count for Gs initialized parameters: 90014147\n",
      "Loading weights...\n",
      "Loading best0 weights from /Work2/Watch_This/ICGAN/ic_gan/pretrained_models_path/icgan_biggan_imagenet_res256...\n",
      "Putting G in eval mode..\n",
      "Number of parameters: 90014340\n",
      "Generating  Index file imagenet_imgs.npz...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:03<00:00,  3.35s/it]\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "\n",
    "vgg16_url = 'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/metrics/vgg16.pkl'\n",
    "vgg16 = metric_utils.get_feature_detector(vgg16_url, device=device)\n",
    "vgg16.to(\"cuda\")\n",
    "vgg16.eval()\n",
    "\n",
    "exp_name = \"%s_%s_%s_res%i%s\" % (\n",
    "        \"icgan\",\n",
    "        \"biggan\",\n",
    "        \"imagenet\",\n",
    "        256,\n",
    "        \"\",\n",
    "    )\n",
    "generator = get_model(\n",
    "        exp_name, \"/Work2/Watch_This/ICGAN/ic_gan/pretrained_models_path\", \"biggan\", device=device\n",
    "    )\n",
    "\n",
    "generator.to(\"cuda\")\n",
    "generator.eval()\n",
    "\n",
    "# files = glob(\"/Work1/imagenet/train/*\")\n",
    "\n",
    "# for file in files:\n",
    "#     os.makedirs(file.replace(\"imagenet\", \"ICGAN_Inversion\"), exist_ok=True)\n",
    "    \n",
    "# files = glob(\"/Work1/imagenet/train/*/*\")\n",
    "\n",
    "# for file in files:\n",
    "#     os.makedirs(file.replace(\"imagenet\", \"ICGAN_Inversion\").replace(\".JPEG\", \"\"), exist_ok=True)\n",
    "\n",
    "dataset = utils.get_dataset_images(\n",
    "        256,\n",
    "        data_path=\"/Work1/imagenet\",\n",
    "        longtail=False,\n",
    "        split=\"train\",\n",
    "        test_part=False,\n",
    "        which_dataset=\"imagenet\",\n",
    "        instance_json=\"\",\n",
    "        stuff_json=\"\",\n",
    "        get_encodings=True,\n",
    "    )\n",
    "\n",
    "generator1 = torch.Generator().manual_seed(42)\n",
    "dataset = torch.utils.data.random_split(dataset, [0.25, 0.25, 0.25, 0.25], generator=generator1)[splitty]\n",
    "\n",
    "loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=41,\n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "num_steps = 100\n",
    "initial_learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8abe09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_63039/1051938831.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z2 = torch.tensor(z1, dtype=torch.float32, device=gpu, requires_grad=True)\n",
      "/home/sarmst/anaconda3/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.012800819252432157\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 46\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m#             mse_loss = l2_criterion(target_img, synth_images)\u001b[39;00m\n\u001b[1;32m     45\u001b[0m             loss \u001b[38;5;241m=\u001b[39m lpips_loss\n\u001b[0;32m---> 46\u001b[0m         \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m         scaler\u001b[38;5;241m.\u001b[39mstep(optimizer)\n\u001b[1;32m     48\u001b[0m         scaler\u001b[38;5;241m.\u001b[39mupdate()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gpu = torch.device(\"cuda\")\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "for ind, data in enumerate(loader):\n",
    "    # if (((ind/len(loader))*100.0) < 34.35133770822817):\n",
    "    #     print(\"skipping\")\n",
    "    #     continue\n",
    "        \n",
    "    with torch.no_grad():\n",
    "#         start_time = time.time()\n",
    "        # data2 = data[0].cuda(gpu, non_blocking=True)\n",
    "        # x_tf1 = data2 * 0.5 + 0.5\n",
    "        # x_tf2 = (x_tf1 - norm_mean) / norm_std\n",
    "        # x_tf3 = torch.nn.functional.upsample(x_tf2, 224, mode=\"bicubic\")\n",
    "        # x_feat1, _ = net(x_tf3)\n",
    "        # x_feat2 =  x_feat1/torch.linalg.norm(x_feat1, dim=1, keepdims=True)\n",
    "\n",
    "        \n",
    "        path = data[3]\n",
    "        data0 = data[0].cuda(gpu, non_blocking=True)\n",
    "        x_feat2 =  data[4].cuda(gpu, non_blocking=True)\n",
    "\n",
    "        z1 = torch.zeros((data0.shape[0], 119)).cuda(gpu)\n",
    "        \n",
    "        x_img_orig = generator(z1, None, x_feat2)\n",
    "\n",
    "        target_img = torch.clamp((data0 * 0.5 + 0.5), 0, 1)*255\n",
    "        target_features = vgg16(F.interpolate(target_img, size=(224, 224)), resize_images=False, return_lpips=True)\n",
    "    \n",
    "    z2 = torch.tensor(z1, dtype=torch.float32, device=gpu, requires_grad=True)\n",
    "    \n",
    "    optimizer = torch.optim.Adam([z2], betas=(0.9, 0.999), lr=initial_learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=0.1, total_iters=20)\n",
    "\n",
    "    # with torch.no_grad():\n",
    "    #     original_synth_image = generator(z2, None, x_feat2)\n",
    "    \n",
    "    for step in range(num_steps):          \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            x_img = generator(z2, None, x_feat2)\n",
    "            synth_images = (x_img + 1) * (255/2)\n",
    "            synth_features = vgg16(F.interpolate(synth_images, size=(224, 224)), resize_images=False, return_lpips=True)\n",
    "            lpips_loss = (target_features - synth_features).square().sum(dim=1).sum()\n",
    "#             mse_loss = l2_criterion(target_img, synth_images)\n",
    "            loss = lpips_loss\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "        \n",
    "    with torch.no_grad():    \n",
    "        # x_img = generator(z2, None, x_feat2)\n",
    "        for ind3 in range(x_img.shape[0]):\n",
    "            folder = path[ind3].replace(\"imagenet\", \"ICGAN_Inversion\").replace(\".JPEG\", \"\")\n",
    "            os.makedirs(folder, exist_ok=True)\n",
    "            file_name = folder.split(\"/\")[-1]\n",
    "            original_image = Image.open(path[ind3]).convert(\"RGB\") \n",
    "            original_image.save(folder + \"/\" + file_name + \"_R.JPEG\")\n",
    "            _gen_img = torchvision.transforms.functional.to_pil_image(torch.clamp((x_img[ind3] * 0.5 + 0.5), 0, 1))\n",
    "            _gen_img.save(folder + \"/\" + file_name + \"_0_G.JPEG\")\n",
    "            # _original_synth_image = torchvision.transforms.functional.to_pil_image(torch.clamp((original_synth_image[ind3] * 0.5 + 0.5), 0, 1))\n",
    "            # _original_synth_image.save(folder + \"/\" + file_name + \"_1_G.JPEG\")\n",
    "    #         break\n",
    "    print((ind/len(loader))*100.0)\n",
    "    torch.save(((ind/len(loader))*100.0), \"Encoder_Inversion\" + str(splitty) + \"_Checkpoint.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b70cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3768f099-412a-45d0-bd8a-1aff884b8574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _original_synth_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa57fb4-bff8-4c1e-8850-324be27c9e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _gen_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7005ee61-effc-45dd-a15a-143a42044be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a0c7d1-3105-45ec-9eef-09578239c832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768c5415-5868-430c-a4fb-f44d377d585f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87079f69-1434-48b7-96e9-9faf09deaf6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
